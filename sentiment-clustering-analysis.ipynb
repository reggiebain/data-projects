{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install contractions\n!pip install flair\n!pip install autocorrect","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-02T21:16:57.860468Z","iopub.execute_input":"2022-01-02T21:16:57.860881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom bs4 import BeautifulSoup\nimport re\nimport nltk\nimport seaborn\nimport matplotlib\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nimport unicodedata\nimport contractions\nfrom flair.models import TextClassifier\nfrom flair.data import Sentence\nfrom autocorrect import Speller","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drug_data = pd.read_csv('../input/case-study-data/case_study_data.tsv', sep='\\t')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(drug_data.shape)\ndrug_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test size of each drug to get one with a lot of entries\ndrug_data.groupby('drugName').count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drug to Analyze: Sertraline\n- Moving Forward analyzing Sertraline, an SSRI originally developed by Pfizer under the brand name 'Zoloft'. \n- Generic forms is called \"Sertraline Hydrochloride\" tablets","metadata":{}},{"cell_type":"code","source":"sertraline_data = drug_data[drug_data['drugName']=='Sertraline']\nsertraline_data.to_csv('sertraline_data.csv')\nsertraline_data = sertraline_data.reset_index(drop=True)\nprint(sertraline_data['review'][3])\nsertraline_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing Notes\n- Reviews contain contractions such as: I've, didn't, wasn't. Should be expanded.\n- Idiosyncratic abbreviations such as: 30's, Dr (instead of doctor),\n- Numbers with units such as '50mg'\n- British colloquialisms such as \"3 stone\"","metadata":{}},{"cell_type":"code","source":"# Define preprocessing function\ndef clean_review(raw_review, remove_stopwords=False):\n    #review_text = BeautifulSoup(raw_review).get_text() # Remove HTML tags\n    review_text = raw_review\n    review_text = unicodedata.normalize('NFKD', review_text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    #spell = Speller(lang='en')\n    #review_text = spell(review_text)\n    review_text = contractions.fix(review_text)\n    #no_specials = re.sub('[^a-zA-z0-9.,!?/:;\\\"\\'\\s]',' ', expand_contractions) # Remove non letters\n    review_text = re.sub('[^a-zA-Z]', ' ', review_text)\n    review_text = review_text.lower() # Lowercase everything\n    review_text = re.sub(' mg ', ' ', review_text)\n    review_text = re.sub(' quot ', ' ', review_text)\n    review_text = re.sub(' olof ', ' zoloft ', review_text)\n    #review_text = re.sub(' zoloft ', ' ', review_text)\n    words = review_text.split() # Tokenize\n    if remove_stopwords:\n        stop_words = set(stopwords.words('english')) # Make stops set for quicker searching\n        words = [word for word in words if not word in stop_words] # Cut stop words\n    return ' '.join(words) # Rejoin from list into passage/string\n\n# Test the method\nprint(sertraline_data['review'][2]+'\\n\\n')\nprint(clean_review(sertraline_data['review'][2]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean All of the Reviews for this Drug\n# Clean all reviews\nnum_reviews = sertraline_data['review'].size\nclean_reviews = []\nfor i in range(0,num_reviews):\n    if((i+1)%100 == 0): print(f'Review {i+1} of {num_reviews}\\n')\n    clean_reviews.append(clean_review(sertraline_data['review'][i], remove_stopwords=True))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model 1 - Use Word2Vec and K Means Clustering on Unlabled Data\n- Preprocess data using Regex and contractions\n- Create word embeddings using word2vec\n- Use clustering since data does not have labels","metadata":{}},{"cell_type":"code","source":"import nltk.data\nfrom gensim.models import word2vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Alternative Approach: Word Vectors**\nUse Google's Word2Vec\n* Define new preprocessing function which OPTIONALLY removes stop words\n* We will use [\\W_]+ so as to keep all alphanumeric and underscore chars\n* This function returns list of words rather than sentences as we did for bag of words appraoch","metadata":{}},{"cell_type":"code","source":"def review_to_wordlist(review, remove_stopwords = False):\n    #review_text = BeautifulSoup(review).get_text()\n    review_text = review\n    review_text = unicodedata.normalize('NFKD', review_text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    review_text = contractions.fix(review_text)\n    #no_specials = re.sub('[^a-zA-z0-9.,!?/:;\\\"\\'\\s]',' ', expand_contractions) # Remove non letters\n    review_text = re.sub('[^a-zA-Z]', ' ', review_text)\n    review_text = review_text.lower() # Lowercase everything\n    review_text = re.sub(' mg ', ' ', review_text)\n    review_text = re.sub(' olof ', ' zoloft ', review_text)\n    review_text = re.sub(' quot ', ' ', review_text)\n    #review_text = re.sub(' zoloft ', ' ', review_text)\n    #spell = Speller(lang='en')\n    #review_text = spell(review_text)\n    words = review_text.lower().split()\n    if remove_stopwords:\n        stops = set(stopwords.words('english'))\n        words = [word for word in words if not word in stops]\n    return words\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\ndef review_to_sentences(review, tokenizer, remove_stopwords = False):\n    raw_sentences = tokenizer.tokenize(review.strip())\n    sentences = []\n    for raw_sentence in raw_sentences:\n        if len(raw_sentence) > 0:\n            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n    return sentences","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = []\nprint('Parts in sentences from unlabled training set')\nfor review in sertraline_data['review']:\n    sentences += review_to_sentences(review, tokenizer, remove_stopwords=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quick check that sentences are created properly\nprint(sertraline_data['review'][0])\nprint(sentences[0])\nprint(len(sentences))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###Default Values for Word2Vec Initializer:\n- sentences=None, corpus_file=None, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), comment=None, max_final_vocab=None, shrink_windows=True)","metadata":{}},{"cell_type":"code","source":"from gensim.models import word2vec\nprint('Training model...')\n\nthe_sentences = sentences\n# Default Values\nw2v_model = word2vec.Word2Vec()\nw2v_model.build_vocab(the_sentences)\nw2v_model.train(the_sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the model by looking at some basic associations\nw2v_model.wv.most_similar(positive=['depressed'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Making Feature Vectors\n- Each customer review should be a list of vector embeddings that can be operated on mathematically\n- We need to create a \"feature set\" out of each review that is the same length even though each has a different number of words","metadata":{}},{"cell_type":"code","source":"print(w2v_model)\nw2v_model.wv['depression']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_vectors = w2v_model.wv\nprint(type(word_vectors.vectors.astype('double')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying K Means Clustering\n- Goal: Divide into 2 clusters: Positive, Negative","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nmodel = KMeans(n_clusters=2, max_iter=1000, random_state=2, n_init=50)\nmodel.fit(X=word_vectors.vectors.astype('double'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_vectors.similar_by_vector(model.cluster_centers_[1], topn=50, restrict_vocab=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#neutral_cluster_center = model.cluster_centers_[1]\npositive_cluster_center = model.cluster_centers_[0]\nnegative_cluster_center = model.cluster_centers_[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculating the Sentiments\n- Total Score / Number of words\n- Positive gets +1 and Negative -1\n- Closeness Score will be the inverse of the minimum distance of the vector to a centroid. This means words that are close to one of the centroids will get a high value of the closenes score since model.transform([a_vector]) yields the distance to all cluster centroids. For example 'celexa' is in cluster [0] the positive cluster...so tranform yields (0.86) which means it's closer to the [0] cluster which is good!. We'll take the minimum of these two.\n- Sentiment coefficient is the closeness score * cluster value (+/- 1) so we have positivity and negativity weighted by how close it is to a centroid.","metadata":{}},{"cell_type":"code","source":"words = pd.DataFrame(w2v_model.wv.index_to_key) # Create data frame where each row is one of the words in corpus\nwords.columns = ['word'] # Name that column words\nwords['vector'] = words.word.apply(lambda x: w2v_model.wv[f'{x}']) # Column of the vectors for each word\nwords['cluster'] = words.vector.apply(lambda x: model.predict([np.array(x)])) # Classify each vector in its cluster\nwords.cluster = words.cluster.apply(lambda x: x[0]) # Grab the cluster number so it's not in bracketss\nwords['cluster_value'] = [1 if i==0 else -1 for i in words.cluster] # 1 for positive guys\nwords['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vector]).min()), axis=1) # Calculate inverse of distancee of word vector to cluster centroid (since 0.99 should be very close 1/(1-0.99) = 1/0.01 > 1)\nwords['sentiment_coeff'] = words.closeness_score * words.cluster_value\nprint(words.shape)\nwords.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words[words['word']=='feel']['vector']\nmodel.transform([words.iloc[766,1]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\nvecs = vectorizer.fit_transform(clean_reviews)\nfeature_names = vectorizer.get_feature_names()\ndense = vecs.todense()\nlst1 = dense.tolist()\ndense_df = pd.DataFrame(lst1, columns=feature_names)\nweight_df = pd.DataFrame(dense_df.T.sum(axis=1), index=None)\nweight_df = weight_df.reset_index()\nweight_df = weight_df.rename(columns={'index': 'word code', 0: 'weight'})\nweight_df['word'] = sorted(vectorizer.vocabulary_.keys())\nweight_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add column to words that has TFDIF value of each word within entire corpus\ntfdif_weighted = words.merge(weight_df, how='inner', left_on='word', right_on='word')\ntfdif_weighted.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfdif_weighted['sentiment_rate'] = tfdif_weighted.apply(lambda x: np.array(x.loc['sentiment_coeff']) * np.array(x.loc['weight']), axis=1)\ntfdif_weighted['prediction'] = (tfdif_weighted.sentiment_rate>0).astype('int8')\ntfdif_weighted.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ncleaned_df = sertraline_data\ncleaned_df['review'] = clean_reviews\n\ntfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\ntfidf.fit(cleaned_df.review)\nfeatures = pd.DataFrame(tfidf.get_feature_names())\ntransformed = tfidf.transform(cleaned_df.review).toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_reviews[0]\ntfdif_weighted[tfdif_weighted.word=='depression'].sentiment_rate.item()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_sentiment(data, sent_dict):\n    total = 0\n    count = 0\n    for word in data.split():\n        if word in list(sent_dict.word):\n            total += sent_dict[sent_dict.word == word].sentiment_rate.item()\n        count += 1\n    avg = total/count\n    if avg < -0.15: sentiment = -1\n    elif avg > 0.15: sentiment = 1\n    return sentiment","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cluster = sertraline_data\ndf_cluster['review'] = clean_reviews\ndf_cluster['sentiment'] = [calc_sentiment(df_cluster['review'][i], tfdif_weighted) for i in range(0, len(clean_reviews))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_cluster = df_cluster.drop(columns=['confidence'])\n# Change +1 and -1 sentiment to positive and negative respectively\ndf_cluster['sentiment_word'] = df_cluster['sentiment'].apply(lambda x: 'POSITIVE' if x == 1 else 'NEGATIVE')\ndf_cluster.head()\n#print(df_cluster.sentiment_word.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_export = df_cluster\ndf_export.to_csv('sertraline_clustering_analysis.csv')\ndf_export.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndf_pie_cluster = df_cluster.sentiment_word.value_counts()\ndf_pie_cluster.index\nprint(df_pie_cluster)\nfig = plt.gcf()\nfig.set_size_inches(7,7)\n#color_dict = dict(zip(np.unique(df_pie_cluster.index), plt.cm.tab10.colors))\n#plt.pie(df_pie_cluster, radius=1, labels = df_pie_cluster.index, autopct=\"%1.1f%%\",\n#       shadow = True,startangle = 90,labeldistance = 1.1,colors=[color_dict[v] for v in df_pie_cluster.index],explode =(0.1,0.1))\nplt.pie(df_pie_cluster, radius=1, labels = df_pie_cluster.index, autopct=\"%1.1f%%\",\n        shadow = True,startangle = 90,labeldistance = 1.1,colors=['lightgreen', 'Pink'],explode =(0.1,0.1))\nplt.axis('equal')\nplt.title(\"Sentiment of Reviews (Clustering)\", fontsize=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making Word Cloud for Positive Sentiment Reviews\nfrom wordcloud import WordCloud\n\nvectorizer_pos = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\nvecs_pos = vectorizer_pos.fit_transform(df_cluster[df_cluster.sentiment_word=='POSITIVE'].review)\nfeature_names = vectorizer_pos.get_feature_names()\ndense_pos = vecs_pos.todense()\nlst_pos = dense_pos.tolist()\ndense_df_pos = pd.DataFrame(lst_pos, columns=feature_names)\ndense_df_pos.T.sum(axis=1)\n\nwordcloud = WordCloud(background_color=\"white\", max_words=50).generate_from_frequencies(dense_df_pos.T.sum(axis=1))\nplt.figure(figsize=(12, 10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Positive Sentiment Word Cloud (Clustering) \\n', fontdict={'fontsize': 40})\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer_neg = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\nvecs_neg = vectorizer_neg.fit_transform(df_cluster[df_cluster.sentiment_word=='NEGATIVE'].review)\nfeature_names_neg = vectorizer_neg.get_feature_names()\ndense_neg = vecs_neg.todense()\nlst_neg = dense_neg.tolist()\ndense_df_neg = pd.DataFrame(lst_neg, columns=feature_names_neg)\ndense_df_neg.T.sum(axis=1)\n\nwordcloud = WordCloud(background_color=\"white\", max_words=50).generate_from_frequencies(dense_df_neg.T.sum(axis=1))\nplt.figure(figsize=(12, 10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Negative Sentiment Word Cloud (Clustering) \\n', fontdict={'fontsize': 40})\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}